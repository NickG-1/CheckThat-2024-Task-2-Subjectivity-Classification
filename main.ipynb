{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckThat! Task 2: Subjectivity Classification\n",
    "\n",
    "## Introduction\n",
    "This notebook is a part of the CheckThat! 2024 Task 2: Subjectivity Classification. The task is to classify the tweets into subjective and objective categories.\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import spacy.symbols\n",
    "\n",
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# customize the tokenizer\n",
    "special_tokens = ['<quote>', '<num>']\n",
    "for token in special_tokens:\n",
    "    nlp.tokenizer.add_special_case(token, [{spacy.symbols.ORTH: token}])\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv('data/train_en.tsv', sep='\\t')\n",
    "validation = pd.read_csv('data/dev_en.tsv', sep='\\t')\n",
    "test = pd.read_csv('data/dev_test_en.tsv', sep='\\t')\n",
    "final_test = pd.read_csv('data/test_en.tsv', sep='\\t')\n",
    "final_test_labels = pd.read_csv('data/test_en_gold.tsv', sep='\\t')\n",
    "\n",
    "# split into X and y\n",
    "X_train = train['sentence']\n",
    "y_train = train['label']\n",
    "\n",
    "X_validation = validation['sentence']\n",
    "y_validation = validation['label']\n",
    "\n",
    "X_test = test['sentence']\n",
    "y_test = test['label']\n",
    "\n",
    "X_final_test = final_test['sentence']\n",
    "y_final_test = final_test_labels['label']\n",
    "\n",
    "# preprocessing\n",
    "def preprocess(sentence):\n",
    "    # lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # replace everything between “ and ” or between \" and \" with <QUOTE>\n",
    "    sentence = re.sub(r'“.*?”|\".*?\"', '<quote>', sentence)\n",
    "    # remove links\n",
    "    sentence = sentence.replace(r'http\\S+', '')\n",
    "    # remove usernames\n",
    "    sentence = sentence.replace(r'@\\S+', '')\n",
    "    # tokenize\n",
    "    tokenized = nlp(sentence)\n",
    "    # remove special characters except for ? !\n",
    "    preserved = ['?', '!']\n",
    "    tokens = []\n",
    "    # filter out stopwords\n",
    "    articles = {'a', 'an', 'the'}\n",
    "    prepositions = {'in', 'on', 'at', 'to', 'from', 'by', 'with', 'about', 'against', 'between', 'during', 'without', 'within', 'among', 'upon'}\n",
    "    conjunctions = {'and', 'but', 'or', 'nor', 'so', 'yet'}\n",
    "    determiners = {'this', 'that', 'these', 'those', 'some', 'any', 'each', 'every', 'all', 'both', 'few', 'many', 'much', 'most', 'other', 'another'}\n",
    "    stropwords = articles | prepositions | conjunctions | determiners\n",
    "\n",
    "    for t in tokenized:\n",
    "        if (not t.is_punct or t.text in preserved) and not t.is_space and t.text not in stropwords:\n",
    "            if t.like_num: # replace numbers with <NUM>\n",
    "                tokens.append('<num>')\n",
    "            else:\n",
    "                tokens.append(t.lemma_) # lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feaure Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "class FeauturesExtractor:\n",
    "    def __init__(self, chi2_features=2000):\n",
    "        # Initialize the TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "        # Initialize Word2Vec\n",
    "        self.word2vec = api.load('word2vec-google-news-300')\n",
    "        # Load the pre-trained SBERT model\n",
    "        self.model = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "        self.sbert = SentenceTransformer(self.model)\n",
    "        # Initialize StandardScaler\n",
    "        self.scaler = StandardScaler()\n",
    "        # Initialize MinMaxScaler SBERT\n",
    "        self.min_max_scaler_sbert = MinMaxScaler()\n",
    "        # Initialize MinMaxScaler Word2Vec\n",
    "        self.min_max_scaler_word2vec = MinMaxScaler()\n",
    "        # Initialize Chi2\n",
    "        k_best_features = chi2_features\n",
    "        self.chi2_selector = SelectKBest(chi2, k=k_best_features)\n",
    "\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.tfidf_vectorizer.fit(X)\n",
    "        tfidf_features = self.tfidf_vectorizer.fit_transform(X).toarray()\n",
    "        sbert_features = self.sbert.encode(X)\n",
    "        sbert_features = self.scaler.fit_transform(sbert_features)\n",
    "        sbert_features_non_negative = self.min_max_scaler_sbert.fit_transform(sbert_features)\n",
    "        word2vec_feautres = self.applyWord2Vec(X)\n",
    "        word2vec_feautres = self.min_max_scaler_word2vec.fit_transform(word2vec_feautres)\n",
    "        combined_features = np.concatenate([tfidf_features, sbert_features_non_negative, word2vec_feautres], axis=1)\n",
    "        chi2_features = self.chi2_selector.fit_transform(combined_features, y)\n",
    "        return chi2_features\n",
    "\n",
    "    def transform(self, X):\n",
    "        tfidf_features = self.tfidf_vectorizer.transform(X).toarray()\n",
    "        sbert_features = self.sbert.encode(X)\n",
    "        sbert_features = self.scaler.transform(sbert_features)\n",
    "        sbert_features_non_negative = self.min_max_scaler_sbert.transform(sbert_features)\n",
    "        word2vec_feautres = self.applyWord2Vec(X)\n",
    "        word2vec_feautres = self.min_max_scaler_word2vec.transform(word2vec_feautres)\n",
    "        combined_features = np.concatenate([tfidf_features, sbert_features_non_negative, word2vec_feautres], axis=1)\n",
    "        chi2_features = self.chi2_selector.transform(combined_features)\n",
    "        return chi2_features\n",
    "    \n",
    "    def applyWord2Vec(self, X):\n",
    "        word2vec_features = []\n",
    "        for sentence in X:\n",
    "            word_vectors = [self.word2vec[word] for word in sentence if word in self.word2vec]\n",
    "            if word_vectors:\n",
    "                word_vectors = np.mean(word_vectors, axis=0)\n",
    "            else:\n",
    "                word_vectors = np.zeros(self.word2vec.vector_size)\n",
    "            word2vec_features.append(word_vectors)\n",
    "        return np.array(word2vec_features)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.tfidf_vectorizer) + ', ' + str(self.model) + ', StandardScaler then MinMaxScaler on sbert, ' + str(self.applyWord2Vec) + ', then MinMaxScaler on word2vec' + ', ' + str(self.chi2_selector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate(y_pred, y_true):\n",
    "    # f1 macro\n",
    "    f1_score_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    # classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    # relative confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    confusion_mat = confusion_mat / confusion_mat.sum(axis=1, keepdims=True)\n",
    "\n",
    "    print('F1 macro:', f1_score_macro, '\\n', 'Classfication report:\\n', report, '\\n', 'Relative confusion matrix:\\n', confusion_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def logEvaluation(y_true, y_pred, model, feature_extractor, preprocessing, additional_info='', filepath='evaluation_log.tsv'):\n",
    "    # Calculate F1 macro score\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Get relative confusion matrix\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    confusion_mat = confusion_mat / confusion_mat.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Get model and feature extractor details\n",
    "    model_info = str(model)\n",
    "    feature_extractor_info = str(feature_extractor)\n",
    "    \n",
    "    # Get preprocessing details\n",
    "    preprocessing_info = preprocessing\n",
    "    \n",
    "    # Get current date and time\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    \n",
    "    # Create a DataFrame for the new log entry\n",
    "    new_entry = pd.DataFrame([{\n",
    "        'f1_macro': f1_macro,\n",
    "        'rel_confusion_matrix': confusion_mat,\n",
    "        'model_info': model_info,\n",
    "        'feature_extractor_info': feature_extractor_info,\n",
    "        'preprocessing_info': preprocessing_info,\n",
    "        'additional_info': additional_info,\n",
    "        'datetime': current_datetime\n",
    "    }])\n",
    "    \n",
    "    # Check if the log file exists\n",
    "    if os.path.exists(filepath):\n",
    "        # Read the existing log file\n",
    "        log_df = pd.read_csv(filepath, sep='\\t')\n",
    "        \n",
    "        # Check if the model or feature extractor has changed\n",
    "        last_entry = log_df.iloc[-1]\n",
    "        if any(last_entry[field] != info for field, info in \n",
    "               [('model_info', model_info), ('feature_extractor_info', feature_extractor_info), \n",
    "                ('preprocessing_info', preprocessing_info), ('additional_info', additional_info)]):\n",
    "            # Append the new entry\n",
    "            log_df = pd.concat([log_df, new_entry], ignore_index=True)\n",
    "    else:\n",
    "        # If the file doesn't exist, create a new DataFrame\n",
    "        log_df = new_entry\n",
    "    \n",
    "    # Write the log to the TSV file\n",
    "    log_df.to_csv(filepath, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro: 0.6983219235264653 \n",
      " Classfication report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         OBJ       0.67      0.75      0.71       106\n",
      "        SUBJ       0.74      0.65      0.69       113\n",
      "\n",
      "    accuracy                           0.70       219\n",
      "   macro avg       0.70      0.70      0.70       219\n",
      "weighted avg       0.70      0.70      0.70       219\n",
      " \n",
      " Relative confusion matrix:\n",
      " [[0.75471698 0.24528302]\n",
      " [0.3539823  0.6460177 ]]\n",
      "F1 macro: 0.6789634146341463 \n",
      " Classfication report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         OBJ       0.65      0.72      0.68       116\n",
      "        SUBJ       0.72      0.64      0.67       127\n",
      "\n",
      "    accuracy                           0.68       243\n",
      "   macro avg       0.68      0.68      0.68       243\n",
      "weighted avg       0.68      0.68      0.68       243\n",
      " \n",
      " Relative confusion matrix:\n",
      " [[0.72413793 0.27586207]\n",
      " [0.36220472 0.63779528]]\n",
      "F1 macro: 0.6388501529051988 \n",
      " Classfication report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         OBJ       0.81      0.87      0.84       362\n",
      "        SUBJ       0.50      0.39      0.44       122\n",
      "\n",
      "    accuracy                           0.75       484\n",
      "   macro avg       0.65      0.63      0.64       484\n",
      "weighted avg       0.73      0.75      0.74       484\n",
      " \n",
      " Relative confusion matrix:\n",
      " [[0.86740331 0.13259669]\n",
      " [0.60655738 0.39344262]]\n"
     ]
    }
   ],
   "source": [
    "fe = FeauturesExtractor()\n",
    "\n",
    "# training\n",
    "X_train = X_train.apply(preprocess)\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# validation\n",
    "X_validation = X_validation.apply(preprocess)\n",
    "X_validation = fe.transform(X_validation)\n",
    "y_pred = model.predict(X_validation)\n",
    "evaluate(y_pred, y_validation)\n",
    "\n",
    "# test\n",
    "X_test = X_test.apply(preprocess)\n",
    "X_test = fe.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "evaluate(y_pred, y_test)\n",
    "\n",
    "# final test\n",
    "X_final_test = X_final_test.apply(preprocess)\n",
    "X_final_test = fe.transform(X_final_test)\n",
    "y_pred = model.predict(X_final_test)\n",
    "evaluate(y_pred, y_final_test)\n",
    "\n",
    "# log evaluation\n",
    "# preprocessing = 'preprocessing: lowercase, replace quotes with <quote>, remove links, usernames and stopwords, lemmatize, remove special characters except for ? and !, replace numbers with <num>'\n",
    "# additional_info = 'erased auxilariy verbs from stopwords'\n",
    "# logEvaluation(y_validation, y_pred, model, fe, preprocessing, additional_info=additional_info, filepath='evaluation_log.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
